# å…¨å‚æ•°å¾®è°ƒæ·±å…¥ç‰ˆ

> é¢å‘æœ‰æœºå™¨å­¦ä¹ åŸºç¡€è¯»è€…çš„æŠ€æœ¯è¯¦è§£

## æ¦‚è¿°

å…¨å‚æ•°å¾®è°ƒï¼ˆFull Fine-tuningï¼‰æ˜¯æŒ‡åœ¨é¢„è®­ç»ƒæ¨¡å‹ $W_{pre}$ çš„åŸºç¡€ä¸Šï¼Œä½¿ç”¨ç›®æ ‡ä»»åŠ¡æ•°æ® $\mathcal{D}_{task}$ï¼Œé€šè¿‡æ¢¯åº¦ä¸‹é™æ›´æ–°æ‰€æœ‰å‚æ•°ï¼š

$$W_{ft} = W_{pre} - \eta \sum_{t=1}^{T} \nabla_{W} \mathcal{L}(f_{W_{pre}}(x_t), y_t)$$

<a id="formula-full-finetune-1"></a>
[ğŸ“– æŸ¥çœ‹å…¬å¼é™„å½•è¯¦è§£](#formula-full-finetune-1-detail)

**å…¬å¼è§£é‡Š**
- **å…¬å¼å«ä¹‰**ï¼šä»é¢„è®­ç»ƒæƒé‡å‡ºå‘ï¼Œç”¨ç›®æ ‡ä»»åŠ¡æ•°æ®åšæ¢¯åº¦ä¸‹é™æ›´æ–°æ‰€æœ‰å‚æ•°ã€‚
- **å˜é‡è¯´æ˜**ï¼š$W_{pre}$ ä¸ºé¢„è®­ç»ƒæƒé‡ï¼›$\eta$ ä¸ºå­¦ä¹ ç‡ï¼›$\mathcal{L}$ ä¸ºæŸå¤±å‡½æ•°ï¼›$T$ ä¸ºè®­ç»ƒæ­¥æ•°ã€‚
- **ç›´è§‰/ä½œç”¨**ï¼šå…¨å‚æ•°æ›´æ–°è®©æ¨¡å‹å……åˆ†é€‚åº”æ–°ä»»åŠ¡ï¼Œæ•ˆæœé€šå¸¸æœ€å¥½ä½†æ˜¾å­˜éœ€æ±‚å¤§ã€‚

å…¶ä¸­ $\eta$ æ˜¯å­¦ä¹ ç‡ï¼Œ$\mathcal{L}$ æ˜¯æŸå¤±å‡½æ•°ï¼Œ$T$ æ˜¯è®­ç»ƒæ­¥æ•°ã€‚

## è®­ç»ƒåŠ¨åŠ›å­¦

### å­¦ä¹ ç‡è°ƒåº¦

å…¨å‚æ•°å¾®è°ƒçš„å­¦ä¹ ç‡è°ƒåº¦è‡³å…³é‡è¦ï¼š

```python
# å¸¸è§çš„å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥
from transformers import get_cosine_schedule_with_warmup

scheduler = get_cosine_schedule_with_warmup(
    optimizer,
    num_warmup_steps=100,      # é¢„çƒ­æ­¥æ•°
    num_training_steps=10000   # æ€»è®­ç»ƒæ­¥æ•°
)
```

**å…³é”®ç‚¹**ï¼š
- **é¢„çƒ­ï¼ˆWarmupï¼‰**ï¼šä»å°å­¦ä¹ ç‡é€æ¸å¢å¤§ï¼Œé¿å…åˆæœŸéœ‡è¡
- **è¡°å‡ï¼ˆDecayï¼‰**ï¼šåæœŸå‡å°å­¦ä¹ ç‡ï¼Œç¨³å®šæ”¶æ•›

### å­¦ä¹ ç‡é€‰æ‹©

| é˜¶æ®µ | å­¦ä¹ ç‡èŒƒå›´ | è¯´æ˜ |
|------|-----------|------|
| é¢„è®­ç»ƒ | 1e-4 ~ 6e-4 | è¾ƒé«˜å­¦ä¹ ç‡ |
| å…¨å‚æ•°å¾®è°ƒ | 1e-5 ~ 5e-5 | é€šå¸¸æ¯”é¢„è®­ç»ƒä½ 10x |
| PEFT å¾®è°ƒ | 1e-4 ~ 1e-3 | å¯ä»¥æ›´é«˜ï¼ˆå‚æ•°å°‘ï¼‰|

## ä¼˜åŒ–æŠ€æœ¯

### æ¢¯åº¦ç´¯ç§¯

æ˜¾å­˜ä¸è¶³æ—¶ï¼Œç”¨æ—¶é—´æ¢ç©ºé—´ï¼š

```python
gradient_accumulation_steps = 4  # ç´¯ç§¯ 4 æ­¥åæ›´æ–°

# æœ‰æ•ˆæ‰¹æ¬¡å¤§å° = batch_size Ã— accumulation_steps
effective_batch_size = 4 Ã— 4 = 16
```

### æ··åˆç²¾åº¦è®­ç»ƒ

ä½¿ç”¨ FP16/BF16 å‡å°‘æ˜¾å­˜ï¼ŒåŠ é€Ÿè®¡ç®—ï¼š

```python
from transformers import TrainingArguments

args = TrainingArguments(
    fp16=True,              # ä½¿ç”¨ FP16
    fp16_opt_level="O1",    # ä¼˜åŒ–çº§åˆ«
    # æˆ–ä½¿ç”¨ BF16ï¼ˆAmpere åŠä»¥ä¸Š GPUï¼‰
    bf16=True,
    tf32=True,              # å¯ç”¨ TF32
)
```

### æ¢¯åº¦æ£€æŸ¥ç‚¹

ä»¥è®¡ç®—æ¢æ˜¾å­˜ï¼š

```python
# åªä¿å­˜éƒ¨åˆ†ä¸­é—´æ¿€æ´»ï¼Œéœ€è¦æ—¶é‡æ–°è®¡ç®—
model.gradient_checkpointing_enable()
```

**æ˜¾å­˜èŠ‚çœ**ï¼šçº¦ 30-50%
**ä»£ä»·**ï¼šè®­ç»ƒé€Ÿåº¦ä¸‹é™çº¦ 20%

### DeepSpeed / FSDP

åˆ†å¸ƒå¼è®­ç»ƒç­–ç•¥ï¼š

| æŠ€æœ¯ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| DDP | æ•°æ®å¹¶è¡Œï¼Œæ¯å¡å¤åˆ¶å®Œæ•´æ¨¡å‹ | å°æ¨¡å‹ |
| FSDP | åˆ†ç‰‡å¹¶è¡Œï¼Œæ¨¡å‹åˆ‡åˆ†åˆ°å¤šå¡ | å¤§æ¨¡å‹ |
| DeepSpeed ZeRO | ä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡ | è¶…å¤§æ¨¡å‹ |

## ç¾éš¾æ€§é—å¿˜

### é—®é¢˜å®šä¹‰

å¾®è°ƒåæ¨¡å‹åœ¨åŸä»»åŠ¡ä¸Šæ€§èƒ½ä¸‹é™ï¼š

$$\text{Forgetting} = \text{Perf}_{pre}(T_{orig}) - \text{Perf}_{ft}(T_{orig})$$

<a id="formula-full-finetune-2"></a>
[ğŸ“– æŸ¥çœ‹å…¬å¼é™„å½•è¯¦è§£](#formula-full-finetune-2-detail)

**å…¬å¼è§£é‡Š**
- **å…¬å¼å«ä¹‰**ï¼šé—å¿˜é‡ = é¢„è®­ç»ƒæ¨¡å‹åœ¨åŸä»»åŠ¡ä¸Šçš„æ€§èƒ½ - å¾®è°ƒååœ¨åŸä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚
- **å˜é‡è¯´æ˜**ï¼š$\text{Perf}_{pre}$ ä¸ºé¢„è®­ç»ƒæ¨¡å‹æ€§èƒ½ï¼›$\text{Perf}_{ft}$ ä¸ºå¾®è°ƒåæ¨¡å‹æ€§èƒ½ï¼›$T_{orig}$ ä¸ºåŸä»»åŠ¡ã€‚
- **ç›´è§‰/ä½œç”¨**ï¼šé‡åŒ–å¾®è°ƒå¯¹åŸæœ‰èƒ½åŠ›çš„æŸå®³ç¨‹åº¦ï¼Œå€¼è¶Šå¤§è¡¨ç¤ºé—å¿˜è¶Šä¸¥é‡ã€‚

### ç¼“è§£ç­–ç•¥

1. **å­¦ä¹ ç‡è°ƒä½**
   ```python
   learning_rate = 2e-5  # æ¯”é¢„è®­ç»ƒä½ 10-20 å€
   ```

2. **æ—©åœï¼ˆEarly Stoppingï¼‰**
   ```python
   # åœ¨éªŒè¯é›†ä¸Šç›‘æ§ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
   early_stopping_patience = 3
   ```

3. **æ··åˆæ•°æ®è®­ç»ƒ**
   ```python
   # æ–°ä»»åŠ¡æ•°æ® + åŸå§‹é¢„è®­ç»ƒæ•°æ®æ··åˆ
   dataset = ConcatDataset([task_dataset, pretrain_subset])
   ```

4. **å¼¹æ€§æƒé‡å›ºåŒ–ï¼ˆEWCï¼‰**
   ```python
   # å¯¹é‡è¦å‚æ•°æ–½åŠ çº¦æŸ
   loss = task_loss + Î» * EWC_penalty
   ```

## è®­ç»ƒé…ç½®æœ€ä½³å®è·µ

### LLaMA-2 7B å…¨å‚æ•°å¾®è°ƒ

```python
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./full-ft-output",

    # æ‰¹æ¬¡è®¾ç½®
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,  # æœ‰æ•ˆæ‰¹æ¬¡ = 16

    # å­¦ä¹ ç‡
    learning_rate=2e-5,
    weight_decay=0.01,

    # è°ƒåº¦
    lr_scheduler_type="cosine",
    warmup_ratio=0.03,

    # ç²¾åº¦
    bf16=True,
    tf32=True,

    # åºåˆ—é•¿åº¦
    max_length=2048,

    # æ¢¯åº¦æ£€æŸ¥ç‚¹
    gradient_checkpointing=True,

    # è®­ç»ƒè½®æ•°
    num_train_epochs=3,

    # ä¿å­˜ç­–ç•¥
    save_strategy="steps",
    save_steps=500,
    save_total_limit=2,

    # æ—¥å¿—
    logging_steps=10,
    eval_steps=500,
)
```

### æ˜¾å­˜ä¼°ç®—

å…¨å‚æ•°å¾®è°ƒæ˜¾å­˜éœ€æ±‚ï¼ˆä¼˜åŒ–å™¨çŠ¶æ€ + æ¢¯åº¦ + æ¿€æ´»ï¼‰ï¼š

$$\text{Memory} \approx 4 \times \text{Params} \times \text{Bytes}$$

<a id="formula-full-finetune-3"></a>
[ğŸ“– æŸ¥çœ‹å…¬å¼é™„å½•è¯¦è§£](#formula-full-finetune-3-detail)

**å…¬å¼è§£é‡Š**
- **å…¬å¼å«ä¹‰**ï¼šå…¨å‚æ•°å¾®è°ƒæ˜¾å­˜çº¦ä¸ºæ¨¡å‹å‚æ•°çš„ 4 å€ï¼ˆä»¥åŒç²¾åº¦è®¡ï¼‰ã€‚
- **å˜é‡è¯´æ˜**ï¼š$\text{Params}$ ä¸ºå‚æ•°æ•°é‡ï¼›$\text{Bytes}$ ä¸ºæ¯ä¸ªæ•°å€¼çš„å­—èŠ‚æ•°ï¼ˆå¦‚ FP16 ä¸º 2ï¼‰ã€‚
- **ç›´è§‰/ä½œç”¨**ï¼šéœ€è¦å­˜å‚¨æ¨¡å‹å‰¯æœ¬ã€æ¢¯åº¦ã€ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆå¦‚ Adam çš„åŠ¨é‡ï¼‰å’Œæ¿€æ´»ï¼Œæ€»å¼€é”€å¤§ã€‚

| æ¨¡å‹ | å‚æ•°é‡ | FP16 æ¨¡å‹ | å…¨å‚æ•°å¾®è°ƒæ˜¾å­˜ |
|------|--------|----------|---------------|
| 7B | 7Ã—10â¹ | 14GB | ~60GB |
| 13B | 13Ã—10â¹ | 26GB | ~100GB |
| 70B | 70Ã—10â¹ | 140GB | ~500GB |

## ä¸ PEFT çš„å¯¹æ¯”å®éªŒ

### å®éªŒè®¾ç½®
- æ¨¡å‹ï¼šLLaMA-2 7B
- ä»»åŠ¡ï¼šæŒ‡ä»¤å¾®è°ƒï¼ˆ52K æ ·æœ¬ï¼‰
- è¯„ä¼°ï¼šMT-Bench

### ç»“æœ

| æ–¹æ³• | å¯è®­ç»ƒå‚æ•° | æ˜¾å­˜ | MT-Bench |
|------|-----------|------|----------|
| å…¨å‚æ•°å¾®è°ƒ | 7B (100%) | 60GB | 6.8 |
| LoRA (r=64) | 70M (1%) | 24GB | 6.5 |
| LoRA (r=16) | 18M (0.3%) | 16GB | 6.3 |

**ç»“è®º**ï¼šå…¨å‚æ•°å¾®è°ƒæ•ˆæœæœ€å¥½ï¼Œä½† LoRA ä»¥ 1% çš„å‚æ•°è¾¾åˆ° 95% çš„æ•ˆæœã€‚

## å®è·µå»ºè®®

1. **å…ˆå°è¯• PEFT**ï¼šå¤§å¤šæ•°åœºæ™¯ LoRA è¶³å¤Ÿ
2. **å…¨å‚æ•°å¾®è°ƒæ¡ä»¶**ï¼š
   - ä»»åŠ¡ä¸é¢„è®­ç»ƒå·®å¼‚å¤§
   - æœ‰å……è¶³æ ‡æ³¨æ•°æ®ï¼ˆ>10Kï¼‰
   - è¿½æ±‚æè‡´æ•ˆæœ
3. **ç›‘æ§é—å¿˜**ï¼šåœ¨åŸå§‹ä»»åŠ¡ä¸Šå®šæœŸè¯„ä¼°
4. **å¤‡ä»½åŸå§‹æ¨¡å‹**ï¼šå…¨å‚æ•°å¾®è°ƒä¸å¯é€†

## å‚è€ƒæ–‡çŒ®

1. Howard & Ruder (2018). *Universal Language Model Fine-tuning for Text Classification*
2. Kirkpatrick et al. (2017). *Overcoming catastrophic forgetting in neural networks*
3. Ouyang et al. (2022). *Training language models to follow instructions with human feedback*

