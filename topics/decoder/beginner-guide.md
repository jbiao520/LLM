# 解码器（Decoder）科普版

> 面向零基础读者的解码器入门指南

## 一句话概括

**解码器就像一个"接龙高手"，根据前面的内容，一个字一个字地生成后续的文字。**

## 从一个问题开始

你有没有玩过"成语接龙"游戏？

- 你说："一心一意"
- 下一个人说："意气风发"
- 再下一个说："发扬光大"

**解码器就在做类似的事情**：根据已经生成的内容，预测下一个字应该是什么。

## 核心概念

### 自回归生成

解码器是"自回归"的，意思是：
- 用已经生成的词，去预测下一个词
- 然后把新词加进去，再预测再下一个词
- 如此循环，直到生成完整内容

```
输入: "今天天气"
预测: "很" → "今天天气很"
预测: "好" → "今天天气很好"
预测: "。" → "今天天气很好。"
```

### 因果性

解码器只能看到"过去"的内容，不能"偷看"未来。

就像你看书时：
- 只能从左到右读
- 不知道后面会发生什么

这叫**因果掩码（Causal Mask）**：后面的内容被"遮住"了。

## 解码器的结构

一个 Transformer 解码器层包含：

1. **掩码自注意力**：只能看前面的词
2. **交叉注意力**：看编码器的输出（如果有的话）
3. **前馈网络**：进一步处理

### 掩码自注意力 vs 普通自注意力

| 类型 | 能看到什么 | 用途 |
|------|-----------|------|
| 普通自注意力 | 整个序列 | 编码器 |
| 掩码自注意力 | 只能看到当前位置之前 | 解码器 |

## GPT：经典的解码器模型

GPT（Generative Pre-trained Transformer）是最著名的解码器模型。

### 工作方式

```
输入: "从前有一座"
↓
GPT 解码器处理
↓
预测下一个词的概率分布
↓
选择概率最高的词（或采样）
↓
输出: "山"
```

### 为什么强大？

1. **海量预训练**：读过大量文本
2. **自监督学习**：不需要人工标注
3. **规模效应**：模型越大，能力越强

## 自回归生成过程

```
步骤 1: 输入 "今天" → 预测 "天气"
步骤 2: 输入 "今天天气" → 预测 "很"
步骤 3: 输入 "今天天气很" → 预测 "好"
步骤 4: 输入 "今天天气很好" → 预测 "。"
步骤 5: 遇到结束符，生成完成
```

这就是 ChatGPT 回答你问题的方式！

## 解码器 vs 编码器

| 特点 | 编码器 | 解码器 |
|------|--------|--------|
| 看的方向 | 双向 | 单向（只看左边） |
| 输出方式 | 一次全部输出 | 逐个生成 |
| 典型任务 | 理解、分类 | 生成、对话 |
| 代表模型 | BERT | GPT |

## KV Cache：加速生成

生成每个新词时，不需要重新计算之前所有词的表示，只需"缓存"之前的 K 和 V，只计算新词的。

这就像：
- 考试时把之前的答案记下来
- 每次只解新题目
- 不用从头做所有题目

## 总结

| 概念 | 通俗理解 |
|------|----------|
| 解码器 | 一个字一个字生成内容 |
| 自回归 | 用前面的内容预测后面 |
| 因果掩码 | 不能看未来的内容 |
| KV Cache | 缓存之前的计算结果 |

## 下一步

- 想了解数学原理？阅读 [深入版](advanced-guide.md)
- 想看代码实现？查看 `examples/` 目录
