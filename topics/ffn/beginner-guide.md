# 前馈网络层（FFN）科普版

> 面向零基础读者 的前馈网络层入门指南

## 一句话概括

**前馈网络层就像一个"信息加工厂"，把输入的信息进行深度加工，提取更复杂的特征。**

## 从一个问题开始

想象你在阅读理解考试中：

1. 首先你理解每个词和它们的关系（这是注意力的工作）
2. 然后你需要"消化"这些信息，提取深层含义（这是前馈网络的工作）

**前馈网络层就是在做这个"消化"的工作。**

## 核心概念

### 什么是前馈网络？

前馈网络是最基础的神经网络结构：
- 输入 → 处理 → 输出
- 没有循环，信息单向流动

在 Transformer 中，每个 FFN 层包含：
1. **升维**：把输入扩展到更大的空间
2. **激活**：引入非线性
3. **降维**：压缩回原来的维度

### "升维-激活-降维"的过程

```
输入 [768维]
    ↓
升维到 [3072维]  ← 扩展 4 倍
    ↓
激活函数 (GELU)  ← 引入非线性
    ↓
降维到 [768维]   ← 压缩回来
    ↓
输出 [768维]
```

### 为什么要这样设计？

1. **升维**：在更大的空间中，更容易找到有意义的特征
2. **激活**：没有它，再多层也只是线性变换
3. **降维**：保持维度一致，方便残差连接

## 类比理解

### 类比1：厨房

- **输入**：原材料（蔬菜、肉类）
- **升维**：把材料切碎、分类（更多的小部件）
- **激活**：烹饪（产生变化）
- **降维**：装盘（重新组合）
- **输出**：成品菜肴

### 类比2：图书馆

- **输入**：书籍
- **升维**：把书的内容展开成章节
- **激活**：理解和分析
- **降维**：总结成摘要
- **输出**：知识精华

## Transformer 中的 FFN

### 位置

在每个 Transformer 层中：

```
输入 → 自注意力 → Add & Norm → FFN → Add & Norm → 输出
```

FFN 紧跟在注意力层之后，对注意力提取的信息进行进一步处理。

### 参数占比

FFN 占了 Transformer 约 2/3 的参数量！

- 注意力层：~1/3 参数
- FFN 层：~2/3 参数

因为 FFN 的两个线性层承担了大部分参数。

## FFN 的变体

### 标准 FFN

最基本的形式：
- 升维 + ReLU/GELU + 降维

### SwiGLU

现代 LLM（如 LLaMA）使用的变体：
- 使用门控机制
- 性能更好
- 计算略复杂

### MoE FFN

混合专家模型的 FFN：
- 多个"专家"网络
- 路由器选择使用哪些专家
- 参数量大但计算量小

## 总结

| 概念 | 通俗理解 |
|------|----------|
| FFN | 信息加工厂 |
| 升维 | 把信息展开 |
| 激活 | 产生变化 |
| 降维 | 重新压缩 |
| SwiGLU | 带门控的高级版本 |

## 下一步

- 想了解数学原理？阅读 [深入版](advanced-guide.md)
- 想看代码实现？查看 `examples/` 目录
