# 归一化（Normalization）���普版

> 面向零基础读者的归一化入门指南

## 一句话概括

**归一化就是把数据"标准化"，让数值保持在合理范围内，帮助模型更好地学习。**

## 从一个问题开始

想象你在考试：
- 语文考了 80 分（满分 100）
- 数学考了 8 分（满分 10）

哪个成绩更好？

实际上都是 80%，但直接看分数，8 和 80 差距很大。如果让计算机处理，它可能会认为语文成绩"远好于"数学成绩。

**归一化就是要解决这个问题：把不同范围的数据放到同一个"尺度"上比较。**

## 核心思想：统一标准

### 生活中的归一化

我们日常生活��其实经常做归一化：

- **百分制**：把不同满分的考试都换算成 100 分
- **汇率换算**：把不同货币换算成同一种货币比较
- **温度换算**：把摄氏度和华氏度换算成统一标准

### 神经网络中的归一化

在神经网络中，每一层的输入数值范围可能差异很大：
- 有的特征在 0 到 1 之间
- 有的特征在 -1000 到 1000 之间

这会导致：
1. **学习困难**：模型不知道该"关注"哪个特征
2. **梯度问题**：数值太大或太小，导致训练不稳定

归一化的做法是：**把数据调整到相似的范围内**，让模型更容易学习。

## 常见的归一化方法

### 1. Batch Normalization（批归一化）

想象一个班级考试：
- 把全班的成绩，调整成平均分 0，标准差 1
- 这样每个人的成绩就是"比平均高多少"或"比平均低多少"

**适用场景**：图像识别等计算机视觉任务

### 2. Layer Normalization（层归一化）

想象一个人的各科成绩：
- 把这个人的语文、数学、英语成绩，调整成平均分 0，标准差 1
- 不看别人的成绩，只看自己的各科成绩

**适用场景**：自然语言处理、Transformer、LLM

### 3. RMS Normalization（均方根归一化）

LayerNorm 的简化版本：
- 不计算平均值，只看数据的"波动程度"
- 计算更简单，效果相近

**适用场景**：现代 LLM（如 LLaMA、GPT-NeoX）

## 为什么 Transformer 用 LayerNorm？

| 方法 | 归一化维度 | 适合场景 |
|------|-----------|---------|
| BatchNorm | 跨样本 | 图像（固定尺寸） |
| LayerNorm | 单样本内 | 文本（变长序列） |

文本序列长度不固定，BatchNorm 处理起来很麻烦。LayerNorm 对每个样本独立处理，完美适配 NLP 任务。

## 一个直观的例子

假设有一个句子 "我 爱 编程"，每个词用 3 维向量表示：

```
原始向量:
我: [10, 0.1, -5]
爱: [0.5, 8, 0.2]
编程: [-3, 0.01, 12]
```

数值范围差异很大（0.01 到 12），LayerNorm 处理后：

```
归一化后:
我: [1.2, -0.5, -0.7]
爱: [-0.6, 1.1, -0.5]
编程: [-0.8, -0.6, 1.4]
```

现在数值都在 -1.5 到 1.5 之间，模型更容易学习。

## 总结

| 概念 | 通俗理解 |
|------|----------|
| 归一化 | 统一数据的标准 |
| BatchNorm | 看全班成绩调整 |
| LayerNorm | 看自己各科成绩调整 |
| RMSNorm | 简化版的 LayerNorm |

## 下一步

- 想了解数学原理？阅读 [深入版](advanced-guide.md)
- 想看代码实现？查看 `examples/` 目录
