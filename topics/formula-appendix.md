# 附录：公式详解

本附录汇总各章节 advanced-guide 中出现的核心公式，提供详细讲解，并支持跳转回原文位置。

## 激活函数

### 万能近似定理（公式 1）

<a id="formula-activation-1-detail"></a>
$$f(x) = W_2 \cdot \sigma(W_1 x + b_1) + b_2$$

**详细讲解（小白友好版）**
- **一句话先懂**：请结合该章节上下文理解该公式的计算目标。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：变量定义沿用原章节中的符号约定。
- **直觉理解**：该公式用于刻画该主题的核心机制与工程作用。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-activation-1)

来源：`topics/activation/advanced-guide.md`

### 数学定义（公式 2）

<a id="formula-activation-2-detail"></a>
$$\text{ReLU}(x) = \max(0, x) = \begin{cases} x & \text{if } x > 0 \\ 0 & \text{if } x \leq 0 \end{cases}$$

**详细讲解（小白友好版）**
- **一句话先懂**：ReLU 把所有负值截断为 0，正值保持不变。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$x$ 是输入标量或向量（逐元素应用）。
- **直觉理解**：像一个“闸门”，只让正信号通过，使激活稀疏且计算简单。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-activation-2)

来源：`topics/activation/advanced-guide.md`

### 导数（公式 3）

<a id="formula-activation-3-detail"></a>
$$\text{ReLU}'(x) = \begin{cases} 1 & \text{if } x > 0 \\ 0 & \text{if } x \leq 0 \end{cases}$$

**详细讲解（小白友好版）**
- **一句话先懂**：ReLU 在正区间导数为 1，负区间导数为 0。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$\text{ReLU}'(x)$ 表示输出对输入的变化率。
- **直觉理解**：正区间梯度稳定、易训练；负区间梯度为 0，可能导致神经元“死亡”。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-activation-3)

来源：`topics/activation/advanced-guide.md`

### 数学定义（公式 4）

<a id="formula-activation-4-detail"></a>
$$\sigma(x) = \frac{1}{1 + e^{-x}}$$

**详细讲解（小白友好版）**
- **一句话先懂**：将任意实数映射到 $(0, 1)$ 的概率式输出。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$x$ 为输入；$e$ 为自然常数。
- **直觉理解**：像一个“概率开关”，常用于二分类输出。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-activation-4)

来源：`topics/activation/advanced-guide.md`

### 导数（公式 5）

<a id="formula-activation-5-detail"></a>
$$\sigma'(x) = \sigma(x)(1 - \sigma(x))$$

**详细讲解（小白友好版）**
- **一句话先懂**：Sigmoid 的梯度与其输出成正相关，在 0 附近最大。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$\sigma'(x)$ 表示输出对输入的敏感度。
- **直觉理解**：当 $\sigma(x)$ 接近 0 或 1 时，梯度趋近 0，容易导致梯度消失。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-activation-5)

来源：`topics/activation/advanced-guide.md`

### 变体：Hard Sigmoid（公式 6）

<a id="formula-activation-6-detail"></a>
$$\text{HardSigmoid}(x) = \max(0, \min(1, \frac{x + 1}{2}))$$

**详细讲解（小白友好版）**
- **一句话先懂**：用分段线性函数近似 Sigmoid，把输出限制在 $[0,1]$。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：当 $x \le -1$ 输出 0；$x \ge 1$ 输出 1；中间线性变化。
- **直觉理解**：牺牲精度换取更快计算和更稳定的梯度。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-activation-6)

来源：`topics/activation/advanced-guide.md`

### 数学定义（公式 7）

<a id="formula-activation-7-detail"></a>
$$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} = 2\sigma(2x) - 1$$

**详细讲解（小白友好版）**
- **一句话先懂**：双曲正切把输入映射到 $(-1, 1)$，是“零中心”的 Sigmoid 变体。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$x$ 为输入；等式右侧说明它与 Sigmoid 的关系。
- **直觉理解**：输出均值更接近 0，有利于优化，但仍可能梯度消失。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-activation-7)

来源：`topics/activation/advanced-guide.md`

### 导数（公式 8）

<a id="formula-activation-8-detail"></a>
$$\tanh'(x) = 1 - \tanh^2(x)$$

**详细讲解（小白友好版）**
- **一句话先懂**：tanh 的梯度由输出值决定，输出越接近 $\pm1$，梯度越小。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$\tanh'(x)$ 是导数，衡量输出对输入的变化率。
- **直觉理解**：当激活饱和时（接近 -1 或 1），学习速度变慢。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-activation-8)

来源：`topics/activation/advanced-guide.md`

### 数学定义（公式 9）

<a id="formula-activation-9-detail"></a>
$$\text{GELU}(x) = x \cdot \Phi(x) = x \cdot P(X \leq x)$$

**详细讲解（小白友好版）**
- **一句话先懂**：GELU 将输入 $x$ 与其被“保留”的概率相乘。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$X \sim \mathcal{N}(0,1)$；$\Phi(x)$ 是标准正态分布 CDF。
- **直觉理解**：输入越大，保留概率越高；输入为负时仍可能保留一部分。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-activation-9)

来源：`topics/activation/advanced-guide.md`

### 数学定义（公式 10）

<a id="formula-activation-10-detail"></a>
$$\Phi(x) = \frac{1}{2}\left[1 + \text{erf}\left(\frac{x}{\sqrt{2}}\right)\right]$$

**详细讲解（小白友好版）**
- **一句话先懂**：标准正态分布的累计概率函数（CDF）。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$\text{erf}(\cdot)$ 是误差函数。
- **直觉理解**：给出 $X \le x$ 的概率，用于平滑地“筛选”输入。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-activation-10)

来源：`topics/activation/advanced-guide.md`

### 近似公式（公式 11）

<a id="formula-activation-11-detail"></a>
$$\text{GELU}(x) \approx 0.5 \cdot x \cdot \left(1 + \tanh\left[\sqrt{\frac{2}{\pi}} \cdot (x + 0.044715 \cdot x^3)\right]\right)$$

**详细讲解（小白友好版）**
- **一句话先懂**：用 $\tanh$ 近似 GELU，避免计算误差函数带来的开销。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$0.044715$ 为经验常数，$x^3$ 提供非线性调节。
- **直觉理解**：在保证形状接近 GELU 的同时提升计算效率。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-activation-11)

来源：`topics/activation/advanced-guide.md`

### 近似公式（公式 12）

<a id="formula-activation-12-detail"></a>
$$\text{SiLU}(x) = x \cdot \sigma(x)$$

**详细讲解（小白友好版）**
- **一句话先懂**：输入 $x$ 与 Sigmoid 门控后的结果相乘。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$\sigma(x)$ 为 Sigmoid。
- **直觉理解**：既保留正值，又在负值区保留小响应，平滑且非单调。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-activation-12)

来源：`topics/activation/advanced-guide.md`

### 导数（公式 13）

<a id="formula-activation-13-detail"></a>
$$\text{GELU}'(x) = \Phi(x) + x \cdot \phi(x)$$

**详细讲解（小白友好版）**
- **一句话先懂**：GELU 的梯度由 CDF 和 PDF 两部分组成。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$\phi(x)$ 是标准正态分布的概率密度函数（PDF）。
- **直觉理解**：梯度随 $x$ 平滑变化，训练更稳定。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-activation-13)

来源：`topics/activation/advanced-guide.md`

### 数学定义（公式 14）

<a id="formula-activation-14-detail"></a>
$$\text{Swish}(x) = x \cdot \sigma(\beta x)$$

**详细讲解（小白友好版）**
- **一句话先懂**：在 Sigmoid 门控后再乘以输入 $x$。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$\beta$ 控制门控的“陡峭度”，可为常数或可学习。
- **直觉理解**：$\beta$ 越大，函数越接近 ReLU；$\beta$ 越小，越平滑。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-activation-14)

来源：`topics/activation/advanced-guide.md`

### 数学定义（公式 15）

<a id="formula-activation-15-detail"></a>
$$\text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{K} e^{x_j}}$$

**详细讲解（小白友好版）**
- **一句话先懂**：把一组实数 $x_i$ 变成概率分布（和为 1）。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$K$ 为类别数；$x_i$ 为第 $i$ 类的打分。
- **直觉理解**：分数越大，指数放大效应越强，概率越高。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-activation-15)

来源：`topics/activation/advanced-guide.md`

### 导数（公式 16）

<a id="formula-activation-16-detail"></a>
$$\frac{\partial \text{Softmax}(x_i)}{\partial x_j} = \text{Softmax}(x_i)(\delta_{ij} - \text{Softmax}(x_j))$$

**详细讲解（小白友好版）**
- **一句话先懂**：请结合该章节上下文理解该公式的计算目标。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：变量定义沿用原章节中的符号约定。
- **直觉理解**：该公式用于刻画该主题的核心机制与工程作用。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-activation-16)

来源：`topics/activation/advanced-guide.md`

### 数值稳定性（公式 17）

<a id="formula-activation-17-detail"></a>
$$\text{Softmax}(x_i) = \frac{e^{x_i - \max(x)}}{\sum_{j=1}^{K} e^{x_j - \max(x)}}$$

**详细讲解（小白友好版）**
- **一句话先懂**：在所有 $x_i$ 上减去最大值不改变结果，但避免指数溢出。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$\max(x)$ 为所有输入中的最大值。
- **直觉理解**：数值稳定性技巧，输出概率不变。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-activation-17)

来源：`topics/activation/advanced-guide.md`

### 温度参数 (Temperature)（公式 18）

<a id="formula-activation-18-detail"></a>
$$\text{Softmax}(x_i, T) = \frac{e^{x_i/T}}{\sum_{j=1}^{K} e^{x_j/T}}$$

**详细讲解（小白友好版）**
- **一句话先懂**：加入温度参数 $T$ 来控制分布的“尖锐程度”。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$T$ 越大，分布越平滑；$T$ 越小，分布越尖锐。
- **直觉理解**：可用于采样时控制随机性。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-activation-18)

来源：`topics/activation/advanced-guide.md`

## 注意力机制

### 数学定义（公式 1）

<a id="formula-attention-1-detail"></a>
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

**详细讲解（小白友好版）**
- **一句话先懂**：请结合该章节上下文理解该公式的计算目标。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：变量定义沿用原章节中的符号约定。
- **直觉理解**：该公式用于刻画该主题的核心机制与工程作用。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-attention-1)

来源：`topics/attention/advanced-guide.md`

### 为什么除以 $\sqrt{d_k}$？（公式 2）

<a id="formula-attention-2-detail"></a>
$$\text{Var}\left(\frac{q \cdot k}{\sqrt{d_k}}\right) = \frac{d_k}{d_k} = 1$$

**详细讲解（小白友好版）**
- **一句话先懂**：缩放后的点积方差被归一到 1。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$q \cdot k$ 是点积；$\text{Var}(\cdot)$ 是方差。
- **直觉理解**：避免 softmax 输入过大导致梯度饱和。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-attention-2)

来源：`topics/attention/advanced-guide.md`

### Self-Attention（公式 3）

<a id="formula-attention-3-detail"></a>
$$Q = XW^Q, \quad K = XW^K, \quad V = XW^V$$

**详细讲解（小白友好版）**
- **一句话先懂**：请结合该章节上下文理解该公式的计算目标。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：变量定义沿用原章节中的符号约定。
- **直觉理解**：该公式用于刻画该主题的核心机制与工程作用。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-attention-3)

来源：`topics/attention/advanced-guide.md`

### 数学定义（公式 4）

<a id="formula-attention-4-detail"></a>
$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O$$

**详细讲解（小白友好版）**
- **一句话先懂**：请结合该章节上下文理解该公式的计算目标。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：变量定义沿用原章节中的符号约定。
- **直觉理解**：该公式用于刻画该主题的核心机制与工程作用。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-attention-4)

来源：`topics/attention/advanced-guide.md`

### 数学定义（公式 5）

<a id="formula-attention-5-detail"></a>
$$\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$

**详细讲解（小白友好版）**
- **一句话先懂**：将注意力分成 $h$ 个子空间并行计算，最后拼接并线性映射。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$h$ 为头数；$W_i^Q, W_i^K, W_i^V$ 为每个头的投影矩阵；$W^O$ 为输出映射。
- **直觉理解**：不同头关注不同关系，提升表达能力。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-attention-5)

来源：`topics/attention/advanced-guide.md`

### Padding Mask（公式 6）

<a id="formula-attention-6-detail"></a>
$$S_{ij} = \begin{cases} S_{ij} & \text{if } j \text{ is valid} \\ -\infty & \text{if } j \text{ is padding} \end{cases}$$

**详细讲解（小白友好版）**
- **一句话先懂**：对 padding 位置施加 $-\infty$，使 softmax 后权重变为 0。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$S_{ij}$ 为注意力分数；$j$ 为被关注位置。
- **直觉理解**：让模型忽略无效填充位置。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-attention-6)

来源：`topics/attention/advanced-guide.md`

### Causal Mask (Look-ahead Mask)（公式 7）

<a id="formula-attention-7-detail"></a>
$$M_{ij} = \begin{cases} 0 & \text{if } j \leq i \\ -\infty & \text{if } j > i \end{cases}$$

**详细讲解（小白友好版）**
- **一句话先懂**：请结合该章节上下文理解该公式的计算目标。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：变量定义沿用原章节中的符号约定。
- **直觉理解**：该公式用于刻画该主题的核心机制与工程作用。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-attention-7)

来源：`topics/attention/advanced-guide.md`

### Causal Mask (Look-ahead Mask)（公式 8）

<a id="formula-attention-8-detail"></a>
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}} + M\right)V$$

**详细讲解（小白友好版）**
- **一句话先懂**：用掩码 $M$ 把未来位置的注意力分数置为 $-\infty$，保证因果性。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$M_{ij}$ 表示位置 $i$ 是否允许看位置 $j$；允许为 0，不允许为 $-\infty$。
- **直觉理解**：解码器生成时只能看“过去”，避免信息泄露。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-attention-8)

来源：`topics/attention/advanced-guide.md`

### Cross-Attention（公式 9）

<a id="formula-attention-9-detail"></a>
$$Q = X_{dec}W^Q, \quad K = X_{enc}W^K, \quad V = X_{enc}W^V$$

**详细讲解（小白友好版）**
- **一句话先懂**：请结合该章节上下文理解该公式的计算目标。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：变量定义沿用原章节中的符号约定。
- **直觉理解**：该公式用于刻画该主题的核心机制与工程作用。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-attention-9)

来源：`topics/attention/advanced-guide.md`

## 归一化

### 数学定义（公式 1）

<a id="formula-normalization-1-detail"></a>
$$\hat{x}_{i,k} = \frac{x_{i,k} - \mu_k}{\sqrt{\sigma_k^2 + \epsilon}}$$

**详细讲解（小白友好版）**
- **一句话先懂**：请结合该章节上下文理解该公式的计算目标。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：变量定义沿用原章节中的符号约定。
- **直觉理解**：该公式用于刻画该主题的核心机制与工程作用。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-normalization-1)

来源：`topics/normalization/advanced-guide.md`

### 数学定义（公式 2）

<a id="formula-normalization-2-detail"></a>
$$y_{i,k} = \gamma_k \hat{x}_{i,k} + \beta_k$$

**详细讲解（小白友好版）**
- **一句话先懂**：对每个特征维度 $k$，减去 batch 内均值、除以标准差，再缩放平移。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$\mu_k, \sigma_k^2$ 为第 $k$ 维在 batch 内的均值和方差；$\gamma_k, \beta_k$ 为可学习的缩放和偏移参数。
- **直觉理解**：将每维特征归一化到标准分布，再通过 $\gamma, \beta$ 恢复表达能力；$\epsilon$ 防止除零。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-normalization-2)

来源：`topics/normalization/advanced-guide.md`

### 梯度计算（公式 3）

<a id="formula-normalization-3-detail"></a>
$$\frac{\partial L}{\partial x_i} = \frac{\gamma}{\sqrt{\sigma^2 + \epsilon}} \left( \frac{\partial L}{\partial y_i} - \frac{1}{m}\sum_{j=1}^{m}\frac{\partial L}{\partial y_j} - \frac{\hat{x}_i}{m}\sum_{j=1}^{m}\frac{\partial L}{\partial y_j}\hat{x}_j \right)$$

**详细讲解（小白友好版）**
- **一句话先懂**：BatchNorm 反向传播时，梯度不仅依赖自身输出，还与 batch 内所有样本相关。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$m$ 为 batch 大小；$\hat{x}_i$ 为归一化后的值；$\partial L / \partial y_j$ 为上游梯度。
- **直觉理解**：归一化操作的梯度会"分散"到整个 batch，增加训练的正规化效果。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-normalization-3)

来源：`topics/normalization/advanced-guide.md`

### 数学定义（公式 4）

<a id="formula-normalization-4-detail"></a>
$$\mu = \frac{1}{H}\sum_{i=1}^{H} x_i$$

**详细讲解（小白友好版）**
- **一句话先懂**：请结合该章节上下文理解该公式的计算目标。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：变量定义沿用原章节中的符号约定。
- **直觉理解**：该公式用于刻画该主题的核心机制与工程作用。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-normalization-4)

来源：`topics/normalization/advanced-guide.md`

### 数学定义（公式 5）

<a id="formula-normalization-5-detail"></a>
$$\sigma^2 = \frac{1}{H}\sum_{i=1}^{H} (x_i - \mu)^2$$

**详细讲解（小白友好版）**
- **一句话先懂**：请结合该章节上下文理解该公式的计算目标。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：变量定义沿用原章节中的符号约定。
- **直觉理解**：该公式用于刻画该主题的核心机制与工程作用。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-normalization-5)

来源：`topics/normalization/advanced-guide.md`

### 数学定义（公式 6）

<a id="formula-normalization-6-detail"></a>
$$y = \frac{\gamma}{\sqrt{\sigma^2 + \epsilon}} \odot (x - \mu) + \beta$$

**详细讲解（小白友好版）**
- **一句话先懂**：对单个样本的所有特征维计算均值和方差，然后归一化。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$H$ 为特征维度数；$\mu, \sigma^2$ 为单样本内的统计量；$\gamma, \beta$ 为可学习参数。
- **直觉理解**：与 BatchNorm 不同，不依赖 batch 大小，适合序列模型和 NLP 任务。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-normalization-6)

来源：`topics/normalization/advanced-guide.md`

### 数学定义（公式 7）

<a id="formula-normalization-7-detail"></a>
$$\text{RMS}(x) = \sqrt{\frac{1}{H}\sum_{i=1}^{H} x_i^2}$$

**详细讲解（小白友好版）**
- **一句话先懂**：请结合该章节上下文理解该公式的计算目标。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：变量定义沿用原章节中的符号约定。
- **直觉理解**：该公式用于刻画该主题的核心机制与工程作用。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-normalization-7)

来源：`topics/normalization/advanced-guide.md`

### 数学定义（公式 8）

<a id="formula-normalization-8-detail"></a>
$$y = \frac{x}{\text{RMS}(x)} \cdot \gamma$$

**详细讲解（小白友好版）**
- **一句话先懂**：只用均方��归一化，省去均值计算，再乘以可学习缩放因子。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$\text{RMS}(x)$ 为均方根值；$\gamma$ 为缩放参数；$H$ 为特征维度数。
- **直觉理解**：简化计算但保持归一化效果，LLaMA 等现代 LLM 广泛使用。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-normalization-8)

来源：`topics/normalization/advanced-guide.md`

### 数学定义（公式 9）

<a id="formula-normalization-9-detail"></a>
$$y = \frac{\gamma}{\sqrt{\frac{1}{H}\sum_{i=1}^{H} x_i^2 + \epsilon}} \cdot x$$

**详细讲解（小白友好版）**
- **一句话先懂**：将 RMSNorm 写成与 LayerNorm 类似的形式，方便对比。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：分母是均方根加 $\epsilon$，分子是缩放参数 $\gamma$。
- **直觉理解**：与 LayerNorm 的区别在于没有 $(x - \mu)$，即不做中心化。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-normalization-9)

来源：`topics/normalization/advanced-guide.md`

## 前馈网络

### 数学定义（公式 1）

<a id="formula-ffn-1-detail"></a>
$$\text{FFN}(x) = \text{GELU}(xW_1 + b_1)W_2 + b_2$$

**详细讲解（小白友好版）**
- **一句话先懂**：先升维到 $d_{ff}$，经 GELU 激活后再降维回 $d_{model}$。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$x$ 为输入向量；$W_1, W_2$ 为权重矩阵；$b_1, b_2$ 为偏置；GELU 为激活函数。
- **直觉理解**：它的核心作用是让模型更容易提取有用信息、压制噪声。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-ffn-1)

来源：`topics/ffn/advanced-guide.md`

### Position-wise 含义（公式 2）

<a id="formula-ffn-2-detail"></a>
$$\text{FFN}(X)_{i,:} = \text{FFN}(X_{i,:})$$

**详细讲解（小白友好版）**
- **一句话先懂**：序列中每个位置的向量独立通过同一个 FFN 变换。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$X_{i,:}$ 为第 $i$ 个位置的向量；$\text{FFN}(X)_{i,:}$ 为其变换后输出。
- **直觉理解**：各位置共享参数但独立处理，保证位置间信息不串扰。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-ffn-2)

来源：`topics/ffn/advanced-guide.md`

### 参数量分析（公式 3）

<a id="formula-ffn-3-detail"></a>
$$P_{FFN} = 2 \times d_{model} \times d_{ff} + d_{model} + d_{ff}$$

**详细讲解（小白友好版）**
- **一句话先懂**：FFN 参数量 = 两个权重矩阵 + 两个偏置向量。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$d_{model}$ 为输入/输出维度；$d_{ff}$ 为中间隐藏维度（通常 $4d$）。
- **直觉理解**：参数量与维度平方成正比，FFN 约占 Transformer 总参数的 2/3。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-ffn-3)

来源：`topics/ffn/advanced-guide.md`

### 参数量分析（公式 4）

<a id="formula-ffn-4-detail"></a>
$$P_{FFN} = 2 \times 768 \times 3072 + 768 + 3072 = 4,722,432$$

**详细讲解（小白友好版）**
- **一句话先懂**：代入具体数值计算 BERT-Base 每层 FFN 的参数量。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$768$ 为隐藏维度；$3072 = 4 \times 768$ 为中间维度。
- **直觉理解**：约 470 万参数，是 BERT-Base 参数量的重要组成部分。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-ffn-4)

来源：`topics/ffn/advanced-guide.md`

### ReLU（公式 5）

<a id="formula-ffn-5-detail"></a>
$$\text{FFN}_{ReLU}(x) = \max(0, xW_1)W_2$$

**详细讲解（小白友好版）**
- **一句话先懂**：用 ReLU 激活，将负值截断为 0。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$xW_1$ 为升维后的结果；$\max(0, \cdot)$ 为逐元素 ReLU。
- **直觉理解**：计算简单，正区间梯度稳定，是原始 Transformer 的选择。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-ffn-5)

来源：`topics/ffn/advanced-guide.md`

### GELU（公式 6）

<a id="formula-ffn-6-detail"></a>
$$\text{FFN}_{GELU}(x) = \text{GELU}(xW_1)W_2$$

**详细讲解（小白友好版）**
- **一句话先懂**：用 GELU 替代 ReLU，在 0 附近更平滑。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：GELU 为高斯误差线性单元，与正态分布 CDF 相关。
- **直觉理解**：平滑过渡避免硬截断，训练更稳定，BERT/GPT 使用。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-ffn-6)

来源：`topics/ffn/advanced-guide.md`

### Swish/SiLU（公式 7）

<a id="formula-ffn-7-detail"></a>
$$\text{FFN}_{Swish}(x) = (xW_1 \odot \sigma(xW_1))W_2$$

**详细讲解（小白友好版）**
- **一句话先懂**：用 Swish 激活，输入与 sigmoid 门控的乘积。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$\odot$ 为逐元素乘法；$\sigma$ 为 Sigmoid 函数。
- **直觉理解**：非单调、有下界无上界，深层网络效果更好。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-ffn-7)

来源：`topics/ffn/advanced-guide.md`

### GLU (Gated Linear Unit)（公式 8）

<a id="formula-ffn-8-detail"></a>
$$\text{GLU}(x) = (xW) \odot \sigma(xV)$$

**详细讲解（小白友好版）**
- **一句话先懂**：两个并行线性变换，一个作为门控信号控制另一个。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$W, V$ 为两组权重；$\sigma(xV)$ 为门控值（0-1）；$\odot$ 为逐元素乘。
- **直觉理解**：门控机制让模型自适应选择哪些信息通过。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-ffn-8)

来源：`topics/ffn/advanced-guide.md`

### SwiGLU（公式 9）

<a id="formula-ffn-9-detail"></a>
$$\text{SwiGLU}(x) = \text{Swish}(xW) \odot (xV)$$

**详细讲解（小白友好版）**
- **一句话先懂**：用 Swish 替代 Sigmoid 作为门控激活。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$\text{Swish}(x) = x \cdot \sigma(x)$；$W, V$ 为两组权重。
- **直觉理解**：Swish 比 Sigmoid 更平滑，LLaMA 等现代 LLM 采用。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-ffn-9)

来源：`topics/ffn/advanced-guide.md`

### SwiGLU（公式 10）

<a id="formula-ffn-10-detail"></a>
$$\text{SwiGLU}(x) = (xW \odot \sigma(xW)) \odot (xV)$$

**详细讲解（小白友好版）**
- **一句话先懂**：请结合该章节上下文理解该公式的计算目标。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：变量定义沿用原章节中的符号约定。
- **直觉理解**：该公式用于刻画该主题的核心机制与工程作用。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-ffn-10)

来源：`topics/ffn/advanced-guide.md`

### GeGLU（公式 11）

<a id="formula-ffn-11-detail"></a>
$$\text{GeGLU}(x) = \text{GELU}(xW) \odot (xV)$$

**详细讲解（小白友好版）**
- **一句话先懂**：用 GELU 替代 Sigmoid 作为门控激活。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：GELU 为高斯误差线性单元；$W, V$ 为两组权重。
- **直觉理解**：GELU 的平滑性带来更好的训练稳定性。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-ffn-11)

来源：`topics/ffn/advanced-guide.md`

## 编码器

### 单层结构（公式 1）

<a id="formula-encoder-1-detail"></a>
$$\text{Output} = \text{LayerNorm}(x + \text{Sublayer}(x))$$

**详细讲解（小白友好版）**
- **一句话先懂**：残差连接将输入 $x$ 与子层输出相加，再通过层归一化得到最终输出。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$x$ 为子层输入；$\text{Sublayer}(x)$ 为注意力或前馈网络的输出；$\text{LayerNorm}$ 为层归一化操作。
- **直觉理解**：残差连接让梯度可以直接流过，缓解深层网络的梯度消失；层归一化稳定每层的输入分布。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-encoder-1)

来源：`topics/encoder/advanced-guide.md`

### 完整公式（公式 2）

<a id="formula-encoder-2-detail"></a>
$$Z = \text{LayerNorm}(X + \text{MultiHead}(X, X, X))$$

**详细讲解（小白友好版）**
- **一句话先懂**：对输入 $X$ 做多头自注意力，加残差后再做层归一化。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$X$ 为输入序列；$\text{MultiHead}$ 为多头注意力；$Z$ 为子层输出。
- **直觉理解**：让每个位置都能"看到"全序列信息，残差和归一化保证训练稳定。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-encoder-2)

来源：`topics/encoder/advanced-guide.md`

### 完整公式（公式 3）

<a id="formula-encoder-3-detail"></a>
$$Y = \text{LayerNorm}(Z + \text{FFN}(Z))$$

**详细讲解（小白友好版）**
- **一句话先懂**：对注意力输出 $Z$ 做前馈网络变换，加残差后再层归一化。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$Z$ 为上一层输出；$\text{FFN}$ 为逐位置前馈网络；$Y$ 为最终层输出。
- **直觉理解**：FFN 对每个位置独立做非线性特征提取，增强表达能力。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-encoder-3)

来源：`topics/encoder/advanced-guide.md`

### 完整公式（公式 4）

<a id="formula-encoder-4-detail"></a>
$$\text{FFN}(x) = \text{GELU}(xW_1 + b_1)W_2 + b_2$$

**详细讲解（小白友好版）**
- **一句话先懂**：先升维再做 GELU 激活，最后降维回原维度。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$W_1$ 升维到 $4d$，$W_2$ 降回 $d$；GELU 为平滑激活函数。
- **直觉理解**：扩展中间维度增加非线性表达能力，是 Transformer 的核心计算模块之一。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-encoder-4)

来源：`topics/encoder/advanced-guide.md`

### 梯度分析（公式 5）

<a id="formula-encoder-5-detail"></a>
$$\frac{\partial L}{\partial x_l} = \frac{\partial L}{\partial x_L} + \sum_{i=l}^{L-1} \frac{\partial L}{\partial f_i}$$

**详细讲解（小白友好版）**
- **一句话先懂**：第 $l$ 层的梯度等于最后一层直接传回的梯度加上中间各子层的梯度贡献。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$x_l$ 为第 $l$ 层输入；$L$ 为总层数；$f_i$ 为第 $i$ 个子层变换。
- **直觉理解**：第一项（直连梯度）保证即使网络很深，梯度也能无损传回，防止梯度消失。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-encoder-5)

来源：`topics/encoder/advanced-guide.md`

### Sinusoidal（原始）（公式 6）

<a id="formula-encoder-6-detail"></a>
$$PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d})$$

**详细讲解（小白友好版）**
- **一句话先懂**：请结合该章节上下文理解该公式的计算目标。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：变量定义沿用原章节中的符号约定。
- **直觉理解**：该公式用于刻画该主题的核心机制与工程作用。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-encoder-6)

来源：`topics/encoder/advanced-guide.md`

### Sinusoidal（原始）（公式 7）

<a id="formula-encoder-7-detail"></a>
$$PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d})$$

**详细讲解（小白友好版）**
- **一句话先懂**：偶数维度用正弦、奇数维度用余弦编码位置信息。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$pos$ 为位置索引；$i$ 为维度索引；$d$ 为嵌入维度。
- **直觉理解**：不同频率的正弦波组合，让模型能区分相对位置关系；无参数且可外推到任意长度。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-encoder-7)

来源：`topics/encoder/advanced-guide.md`

### 输入表示（公式 8）

<a id="formula-encoder-8-detail"></a>
$$\text{Input} = \text{Token Embedding} + \text{Segment Embedding} + \text{Position Embedding}$$

**详细讲解（小白友好版）**
- **一句话先懂**：BERT 输入由三种嵌入相加得到：词嵌入、句子嵌入、位置嵌入。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：Token Embedding 表示词本身；Segment Embedding 区分句子对；Position Embedding 编码位置。
- **直觉理解**：将词语义、句子归属、位置信息统一编码到同一向量空间。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-encoder-8)

来源：`topics/encoder/advanced-guide.md`

### Masked Language Model (MLM)（公式 9）

<a id="formula-encoder-9-detail"></a>
$$P(mask\_token | context) = \text{softmax}(h \cdot E^T)$$

**详细讲解（小白友好版）**
- **一句话先懂**：用 `[MASK]` 位置的隐藏状态 $h$ 与词嵌入矩阵 $E$ 计算相似度，再用 softmax 得到词概率分布。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$h$ 为 `[MASK]` 位置的表示向量；$E$ 为词嵌入矩阵；$E^T$ 为其转置。
- **直觉理解**：将隐藏状态映射回词表空间，预测被遮盖的原始词。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-encoder-9)

来源：`topics/encoder/advanced-guide.md`

### Next Sentence Prediction (NSP)（公式 10）

<a id="formula-encoder-10-detail"></a>
$$P(isNext) = \text{sigmoid}(h_{[CLS]} \cdot w)$$

**详细讲解（小白友好版）**
- **一句话先懂**：用 `[CLS]` 位置的隐藏状态做二分类，判断两句子是否连续。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$h_{[CLS]}$ 为 `[CLS]` token 的表示；$w$ 为分类权重向量；$\text{sigmoid}$ 将输出压缩到 $(0,1)$。
- **直觉理解**：`[CLS]` 聚合了全句信息，用于句子级别的预测任务。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-encoder-10)

来源：`topics/encoder/advanced-guide.md`

### 1. 双向上下文（公式 11）

<a id="formula-encoder-11-detail"></a>
$$h_i = f(x_1, x_2, ..., x_n)$$

**详细讲解（小白友好版）**
- **一句话先懂**：编码器中每个位置的表示都依赖全序列信息。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$h_i$ 为位置 $i$ 的输出表示；$x_1, ..., x_n$ 为序列所有位置的输入。
- **直觉理解**：双向建模让每个词都能"看到"前后文，更适合理解任务。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-encoder-11)

来源：`topics/encoder/advanced-guide.md`

### 参数共享（公式 12）

<a id="formula-encoder-12-detail"></a>
$$\text{MLM\_logits} = h \cdot E^T + b$$

**详细讲解（小白友好版）**
- **一句话先懂**：预测被遮盖词时，直接复用输入词嵌入矩阵作为输出投影。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$h$ 为隐藏状态；$E^T$ 为词嵌入矩阵转置；$b$ 为偏置。
- **直觉理解**：共享矩阵减少参数量，同时输入和输出语义对齐更一致。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-encoder-12)

来源：`topics/encoder/advanced-guide.md`

## 解码器

### 数学公式（公式 1）

<a id="formula-decoder-1-detail"></a>
$$h_l' = \text{LayerNorm}(h_{l-1} + \text{MaskedMHSA}(h_{l-1}))$$

**详细讲解（小白友好版）**
- **一句话先懂**：请结合该章节上下文理解该公式的计算目标。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：变量定义沿用原章节中的符号约定。
- **直觉理解**：该公式用于刻画该主题的核心机制与工程作用。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-decoder-1)

来源：`topics/decoder/advanced-guide.md`

### 数学公式（公式 2）

<a id="formula-decoder-2-detail"></a>
$$h_l'' = \text{LayerNorm}(h_l' + \text{CrossAttention}(h_l', c))$$

**详细讲解（小白友好版）**
- **一句话先懂**：请结合该章节上下文理解该公式的计算目标。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：变量定义沿用原章节中的符号约定。
- **直觉理解**：该公式用于刻画该主题的核心机制与工程作用。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-decoder-2)

来源：`topics/decoder/advanced-guide.md`

### 数学公式（公式 3）

<a id="formula-decoder-3-detail"></a>
$$h_l = \text{LayerNorm}(h_l'' + \text{FFN}(h_l''))$$

**详细讲解（小白友好版）**
- **一句话先懂**：请结合该章节上下文理解该公式的计算目标。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：变量定义沿用原章节中的符号约定。
- **直觉理解**：该公式用于刻画该主题的核心机制与工程作用。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-decoder-3)

来源：`topics/decoder/advanced-guide.md`

### 因果掩码（公式 4）

<a id="formula-decoder-4-detail"></a>
$$M_{ij} = \begin{cases} 0 & \text{if } j \leq i \\ -\infty & \text{if } j > i \end{cases}$$

**详细讲解（小白友好版）**
- **一句话先懂**：请结合该章节上下文理解该公式的计算目标。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：变量定义沿用原章节中的符号约定。
- **直觉理解**：该公式用于刻画该主题的核心机制与工程作用。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-decoder-4)

来源：`topics/decoder/advanced-guide.md`

### 因果掩码（公式 5）

<a id="formula-decoder-5-detail"></a>
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}} + M\right)V$$

**详细讲解（小白友好版）**
- **一句话先懂**：用掩码 $M$ 把未来位置的注意力分数设为 $-\infty$，softmax 后权重为 0。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$M_{ij}$ 表示位置 $i$ 是否能看位置 $j$；$d_k$ 为键维度。
- **直觉理解**：确保解码时只使用历史信息，符合自回归生成。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-decoder-5)

来源：`topics/decoder/advanced-guide.md`

### 注意力矩阵形状（公式 6）

<a id="formula-decoder-6-detail"></a>
$$M = \begin{bmatrix} 0 & -\infty & -\infty & -\infty \\ 0 & 0 & -\infty & -\infty \\ 0 & 0 & 0 & -\infty \\ 0 & 0 & 0 & 0 \end{bmatrix}$$

**详细讲解（小白友好版）**
- **一句话先懂**：请结合该章节上下文理解该公式的计算目标。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：变量定义沿用原章节中的符号约定。
- **直觉理解**：该公式用于刻画该主题的核心机制与工程作用。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-decoder-6)

来源：`topics/decoder/advanced-guide.md`

### 生成过程（公式 7）

<a id="formula-decoder-7-detail"></a>
$$P(x_{t+1} | x_{1:t}) = \text{softmax}(Wh_t)$$

**详细讲解（小白友好版）**
- **一句话先懂**：请结合该章节上下文理解该公式的计算目标。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：变量定义沿用原章节中的符号约定。
- **直觉理解**：该公式用于刻画该主题的核心机制与工程作用。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-decoder-7)

来源：`topics/decoder/advanced-guide.md`

### 完整序列概率（公式 8）

<a id="formula-decoder-8-detail"></a>
$$P(x_1, ..., x_T) = \prod_{t=1}^{T} P(x_t | x_{1:t-1})$$

**详细讲解（小白友好版）**
- **一句话先懂**：整句概率等于每一步条件概率的连乘。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$T$ 为序列长度；$x_{1:t-1}$ 为历史上下文。
- **直觉理解**：体现自回归分解假设，逐步生成整句。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-decoder-8)

来源：`topics/decoder/advanced-guide.md`

### Greedy Decoding（公式 9）

<a id="formula-decoder-9-detail"></a>
$$x_{t+1} = \arg\max_{x} P(x | x_{1:t})$$

**详细讲解（小白友好版）**
- **一句话先懂**：每一步都选取概率最高的 token。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$\arg\max$ 表示取得使概率最大的 $x$。
- **直觉理解**：确定性输出，但可能缺乏多样性。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-decoder-9)

来源：`topics/decoder/advanced-guide.md`

### Sampling（公式 10）

<a id="formula-decoder-10-detail"></a>
$$x_{t+1} \sim P(x | x_{1:t})$$

**详细讲解（小白友好版）**
- **一句话先懂**：按照概率分布随机采样下一个 token。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$\sim$ 表示“服从该分布采样”。
- **直觉理解**：提高多样性，但可能引入噪声。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-decoder-10)

来源：`topics/decoder/advanced-guide.md`

### Temperature Sampling（公式 11）

<a id="formula-decoder-11-detail"></a>
$$P_{temp}(x | x_{1:t}) = \frac{\exp(\log P(x) / T)}{\sum_{x'} \exp(\log P(x') / T)}$$

**详细讲解（小白友好版）**
- **一句话先懂**：请结合该章节上下文理解该公式的计算目标。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：变量定义沿用原章节中的符号约定。
- **直觉理解**：该公式用于刻画该主题的核心机制与工程作用。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-decoder-11)

来源：`topics/decoder/advanced-guide.md`

### 解决方案（公式 12）

<a id="formula-decoder-12-detail"></a>
$$K_{1:t} = [K_1, K_2, ..., K_t]$$

**详细讲解（小白友好版）**
- **一句话先懂**：请结合该章节上下文理解该公式的计算目标。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：变量定义沿用原章节中的符号约定。
- **直觉理解**：该公式用于刻画该主题的核心机制与工程作用。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-decoder-12)

来源：`topics/decoder/advanced-guide.md`

### 解决方案（公式 13）

<a id="formula-decoder-13-detail"></a>
$$V_{1:t} = [V_1, V_2, ..., V_t]$$

**详细讲解（小白友好版）**
- **一句话先懂**：请结合该章节上下文理解该公式的计算目标。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：变量定义沿用原章节中的符号约定。
- **直觉理解**：该公式用于刻画该主题的核心机制与工程作用。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-decoder-13)

来源：`topics/decoder/advanced-guide.md`

### 内存分析（公式 14）

<a id="formula-decoder-14-detail"></a>
$$M_{cache} = 2 \times L \times B \times n_{heads} \times d_{head} \times (n_{ctx} + n_{gen})$$

**详细讲解（小白友好版）**
- **一句话先懂**：请结合该章节上下文理解该公式的计算目标。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：变量定义沿用原章节中的符号约定。
- **直觉理解**：该公式用于刻画该主题的核心机制与工程作用。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-decoder-14)

来源：`topics/decoder/advanced-guide.md`

### 训练目标（公式 15）

<a id="formula-decoder-15-detail"></a>
$$\mathcal{L} = -\sum_{t=1}^{T} \log P(x_t | x_{1:t-1})$$

**详细讲解（小白友好版）**
- **一句话先懂**：对每一步的正确 token 概率取对数并求和，取负号得到损失。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$P(x_t | x_{1:t-1})$ 为模型在第 $t$ 步对真实 token 的概率。
- **直觉理解**：概率越大，损失越小；训练目标是最大化真实序列概率。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-decoder-15)

来源：`topics/decoder/advanced-guide.md`

### Cross-Attention（用于 Encoder-Decoder）（公式 16）

<a id="formula-decoder-16-detail"></a>
$$\text{CrossAttention}(Q_d, K_e, V_e) = \text{softmax}\left(\frac{Q_d K_e^T}{\sqrt{d_k}}\right)V_e$$

**详细讲解（小白友好版）**
- **一句话先懂**：请结合该章节上下文理解该公式的计算目标。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：变量定义沿用原章节中的符号约定。
- **直觉理解**：该公式用于刻画该主题的核心机制与工程作用。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-decoder-16)

来源：`topics/decoder/advanced-guide.md`

## 混合专家模型

### 基本定义（公式 1）

<a id="formula-moe-1-detail"></a>
$$\text{MoE}(x) = \sum_{i=1}^{n} G(x)_i \cdot E_i(x)$$

**详细讲解（小白友好版）**
- **一句话先懂**：所有专家的输出加权求和，权重由路由器 $G(x)$ 决定。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$n$ 为专家总数；$G(x)_i$ 为第 $i$ 个专家的权重；$E_i(x)$ 为第 $i$ 个专家的输出。
- **直觉理解**：不同专家专注于不同类型的输入，路由器决定每个输入应由哪些专家处理。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-moe-1)

来源：`topics/moe/advanced-guide.md`

### 稀疏 MoE (Sparse MoE)（公式 2）

<a id="formula-moe-2-detail"></a>
$$\text{SparseMoE}(x) = \sum_{i \in \text{TopK}(G(x))} G(x)_i \cdot E_i(x)$$

**详细讲解（小白友好版）**
- **一句话先懂**：只取路由分数最高的 $k$ 个专家参与计算，其余权重置零。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$\text{TopK}(G(x))$ 返回路由分数最高的 $k$ 个专家索引；$k$ 通常为 1 或 2。
- **直觉理解**：稀疏激活大幅减少计算量，同时保持模型容量。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-moe-2)

来源：`topics/moe/advanced-guide.md`

### 1. Softmax 路由（公式 3）

<a id="formula-moe-3-detail"></a>
$$G(x) = \text{Softmax}(x \cdot W_g)$$

**详细讲解（小白友好版）**
- **一句话先懂**：将输入 $x$ 线性映射到 $n$ 维，再经 softmax 得到每个专家的权重。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$W_g \in \mathbb{R}^{d \times n}$ 为路由器权重；$x$ 为输入向量。
- **直觉理解**：权重和为 1，可解释为"选择专家的概率分布"。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-moe-3)

来源：`topics/moe/advanced-guide.md`

### 2. Top-K 路由（公式 4）

<a id="formula-moe-4-detail"></a>
$$G(x)_i = \begin{cases} \frac{\exp((x \cdot W_g)_i)}{\sum_{j \in \text{TopK}} \exp((x \cdot W_g)_j)} & \text{if } i \in \text{TopK} \\ 0 & \text{otherwise} \end{cases}$$

**详细讲解（小白友好版）**
- **一句话先懂**：只保留 top-k 专家的权重，其余置零，再对保留的权重归一化。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$\text{TopK}$ 为分数最高的 $k$ 个专家集合；$\exp(\cdot)$ 保证权重非负。
- **直觉理解**：只激活少数专家，计算量与 $k$ 成正比而非专家总数。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-moe-4)

来源：`topics/moe/advanced-guide.md`

### 3. Top-K with Noise（带噪声的 Top-K）（公式 5）

<a id="formula-moe-5-detail"></a>
$$H(x) = x \cdot W_g + \text{StandardNormal}() \cdot \text{Softplus}(x \cdot W_{noise})$$

**详细讲解（小白友好版）**
- **一句话先懂**：请结合该章节上下文理解该公式的计算目标。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：变量定义沿用原章节中的符号约定。
- **直觉理解**：该公式用于刻画该主题的核心机制与工程作用。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-moe-5)

来源：`topics/moe/advanced-guide.md`

### 3. Top-K with Noise（带噪声的 Top-K）（公式 6）

<a id="formula-moe-6-detail"></a>
$$G(x) = \text{Softmax}(\text{TopK}(H(x)))$$

**详细讲解（小白友好版）**
- **一句话先懂**：在路由分数上添加可学习的噪声，增加路由的探索性。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$\text{StandardNormal}()$ 为标准正态采样；$\text{Softplus}(x \cdot W_{noise})$ 控制噪声大小。
- **直觉理解**：噪声让路由在训练时探索更多专家组合，避免过早收敛到次优分配。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-moe-6)

来源：`topics/moe/advanced-guide.md`

### Load Balancing Loss（公式 7）

<a id="formula-moe-7-detail"></a>
$$L_{aux} = \alpha \cdot n \cdot \sum_{i=1}^{n} f_i \cdot P_i$$

**详细讲解（小白友好版）**
- **一句话先懂**：惩罚专家负载不均衡，鼓励所有专家被均匀使用。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$f_i$ 为实际路由到专家 $i$ 的 token 比例；$P_i$ 为专家 $i$ 的平均路由概率；$\alpha$ 为调节系数。
- **直觉理解**：当 $f_i = P_i = 1/n$ 时损失最小，即所有专家被均匀激活。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-moe-7)

来源：`topics/moe/advanced-guide.md`

### 完整损失（公式 8）

<a id="formula-moe-8-detail"></a>
$$L_{total} = L_{task} + L_{aux}$$

**详细讲解（小白友好版）**
- **一句话先懂**：总损失 = 任务损失 + 负载均衡辅助损失。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$L_{task}$ 为目标任务损失（如语言模型损失）；$L_{aux}$ 为负载均衡损失。
- **直觉理解**：辅助损失引导路由器��匀使用所有专家，避免"专家坍塌"。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-moe-8)

来源：`topics/moe/advanced-guide.md`

### 专家容量（Expert Capacity）（公式 9）

<a id="formula-moe-9-detail"></a>
$$\text{capacity}_i = \frac{N \cdot k}{n} \cdot \text{capacity\_factor}$$

**详细讲解（小白友好版）**
- **一句话先懂**：每个专家的容量 = 总 token 数 × 每个 token 激活的专家数 ÷ 专家总数 × 容量因子。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$N$ 为总 token 数；$k$ 为每个 token 激活的专家数；$n$ 为专家总数。
- **直觉理解**：容量因子 > 1 提供缓冲，避免因专家过载而丢弃 token。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-moe-9)

来源：`topics/moe/advanced-guide.md`

## 词嵌入

### 概述（公式 1）

<a id="formula-word-embedding-1-detail"></a>
$$f: V \rightarrow \mathbb{R}^d$$

**详细讲解（小白友好版）**
- **一句话先懂**：将词汇表中的每个词映射到一个 $d$ 维实数向量。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$V$ 为词汇表（离散符号集合）；$\mathbb{R}^d$ 为 $d$ 维实数向量空间。
- **直觉理解**：用稠密向量表示词语，使语义相近的词在向量空间中距离也相近。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-word-embedding-1)

来源：`topics/embedding/word-embedding/advanced-guide.md`

### 1. Skip-gram 模型（公式 2）

<a id="formula-word-embedding-2-detail"></a>
$$\frac{1}{T} \sum_{t=1}^{T} \sum_{-c \leq j \leq c, j \neq 0} \log p(w_{t+j} | w_t)$$

**详细讲解（小白友好版）**
- **一句话先懂**：用中心词预测上下文词，最大化所有位置的对数概率之和。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$T$ 为序列长度；$c$ 为上下文窗口大小；$p(w_{t+j} | w_t)$ 为用中心词预测上下文词的概率。
- **直觉理解**：让中心词的向量能"预测"周围词，语义相近的词会有相似的向量。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-word-embedding-2)

来源：`topics/embedding/word-embedding/advanced-guide.md`

### 1. Skip-gram 模型（公式 3）

<a id="formula-word-embedding-3-detail"></a>
$$p(w_o | w_i) = \frac{\exp(v_{w_o}^T v_{w_i})}{\sum_{w=1}^{W} \exp(v_w^T v_{w_i})}$$

**详细讲解（小白友好版）**
- **一句话先懂**：用两个词向量的点积计算相似度，再经 softmax 归一化为概率。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$v_{w_o}$ 为上下文词向量；$v_{w_i}$ 为中心词向量；$W$ 为词表大小。
- **直觉理解**：点积衡量相似度，相似度高的词对预测概率更大。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-word-embedding-3)

来源：`topics/embedding/word-embedding/advanced-guide.md`

### 2. 负采样 (Negative Sampling)（公式 4）

<a id="formula-word-embedding-4-detail"></a>
$$\log p(w_o | w_i) = \log \sigma(v_{w_o}^T v_{w_i}) + \sum_{k=1}^{K} \mathbb{E}_{w_k \sim P_n(w)} [\log \sigma(-v_{w_k}^T v_{w_i})]$$

**详细讲解（小白友好版）**
- **一句话先懂**：正样本（真实上下文）概率最大化 + 负样本（噪声词）概率最小化。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$\sigma$ 为 sigmoid 函数；$K$ 为负样本数；$P_n(w)$ 为噪声分布；$v_{w_k}$ 为负样本词向量。
- **直觉理解**：只需计算 $K+1$ 个词而非整个词表，大幅加速训练。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-word-embedding-4)

来源：`topics/embedding/word-embedding/advanced-guide.md`

### 3. CBOW 模型（公式 5）

<a id="formula-word-embedding-5-detail"></a>
$$p(w_t | \text{context}) = \frac{\exp(v_{w_t}^T \cdot \bar{v}_{\text{context}})}{\sum_{w=1}^{W} \exp(v_w^T \cdot \bar{v}_{\text{context}})}$$

**详细讲解（小白友好版）**
- **一句话先懂**：用上下文词向量的平均来预测中心词的概率分布。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$\bar{v}_{\text{context}}$ 为上下文词向量的平均；$v_{w_t}$ 为中心词向量；$W$ 为词表大小。
- **直觉理解**：上下文信息聚合成一个表示，再预测最可能的中心词。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-word-embedding-5)

来源：`topics/embedding/word-embedding/advanced-guide.md`

### 目标函数（公式 6）

<a id="formula-word-embedding-6-detail"></a>
$$J = \sum_{i,j=1}^{V} f(X_{ij}) (w_i^T \tilde{w}_j + b_i + \tilde{b}_j - \log X_{ij})^2$$

**详细讲解（小白友好版）**
- **一句话先懂**：让词向量的点积 + 偏置逼近共现次数的对数。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$X_{ij}$ 为词 $i$ 和 $j$ 的共现次数；$w_i, \tilde{w}_j$ 为中心词和上下文词向量；$f(X_{ij})$ 为权重函数。
- **直觉理解**：利用全局共现统计，学习能捕捉词间关系的向量表示。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-word-embedding-6)

来源：`topics/embedding/word-embedding/advanced-guide.md`

### 目标函数（公式 7）

<a id="formula-word-embedding-7-detail"></a>
$$f(x) = \begin{cases} (x/x_{\max})^\alpha & \text{if } x < x_{\max} \\ 1 & \text{otherwise} \end{cases}$$

**详细讲解（小白友好版）**
- **一句话先懂**：对低频共现给予较小权重，高频共现权重封顶为 1。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$x_{\max}$ 为截断阈值；$\alpha$ 为幂次参数（通常 0.75）。
- **直觉理解**：防止高频词（如"的"）主导训练目标。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-word-embedding-7)

来源：`topics/embedding/word-embedding/advanced-guide.md`

### 1. 余弦相似度（公式 8）

<a id="formula-word-embedding-8-detail"></a>
$$\text{sim}(w_1, w_2) = \frac{v_{w_1} \cdot v_{w_2}}{\|v_{w_1}\| \|v_{w_2}\|}$$

**详细讲解（小白友好版）**
- **一句话先懂**：用两个向量的夹角余弦值衡量相似度，范围 $[-1, 1]$。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$v_{w_1}, v_{w_2}$ 为两个词的向量；$\|\cdot\|$ 为向量模长。
- **直觉理解**：归一化后只看方向不看大小，语义相近的词夹角小、余弦值大。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-word-embedding-8)

来源：`topics/embedding/word-embedding/advanced-guide.md`

### 2. 类比关系（公式 9）

<a id="formula-word-embedding-9-detail"></a>
$$\vec{v}_{king} - \vec{v}_{man} + \vec{v}_{woman} \approx \vec{v}_{queen}$$

**详细讲解（小白友好版）**
- **一句话先懂**：词向量空间中的线性运算可表示语义类比关系。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$\vec{v}_{word}$ 为词的向量表示；减法和加法为向量运算。
- **直觉理解**："king - man + woman" 体现了"男性→女性"的语义方向，应用到 king 得到 queen。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-word-embedding-9)

来源：`topics/embedding/word-embedding/advanced-guide.md`

## 位置嵌入

### 为什么需要位置信息？（公式 1）

<a id="formula-position-embedding-1-detail"></a>
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V$$

**详细讲解（小白友好版）**
- **一句话先懂**：注意力的输出只依赖 Q、K、V 的值，与序列顺序无关。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$Q, K, V$ 为查询、键、值矩阵；$d_k$ 为键维度。
- **直觉理解**：打乱输入顺序只改变输出顺序，不改变内容；因此需要额外注入位置信息。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-position-embedding-1)

来源：`topics/embedding/position-embedding/advanced-guide.md`

### Sinusoidal Position Encoding（公式 2）

<a id="formula-position-embedding-2-detail"></a>
$$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$

**详细讲解（小白友好版）**
- **一句话先懂**：请结合该章节上下文理解该公式的计算目标。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：变量定义沿用原章节中的符号约定。
- **直觉理解**：该公式用于刻画该主题的核心机制与工程作用。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-position-embedding-2)

来源：`topics/embedding/position-embedding/advanced-guide.md`

### Sinusoidal Position Encoding（公式 3）

<a id="formula-position-embedding-3-detail"></a>
$$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$

**详细讲解（小白友好版）**
- **一句话先懂**：偶数维度用正弦、奇数维度用余弦编码位置信息，不同维度使用不同频率。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$pos$ 为位置索引；$i$ 为维度索引；$d_{model}$ 为嵌入维度；$10000$ 为底数控制频率范围。
- **直觉理解**：每个位置有唯一编码；低维捕捉局部位置，高维捕捉全局位置；可外推到训练未见长度。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-position-embedding-3)

来源：`topics/embedding/position-embedding/advanced-guide.md`

### 3. 相对位置关系（公式 4）

<a id="formula-position-embedding-4-detail"></a>
$$\sin(x + k) = \sin(x)\cos(k) + \cos(x)\sin(k)$$

**详细讲解（小白友好版）**
- **一句话先懂**：正弦函数的加法公式表明位置 $pos+k$ 的编码可由位置 $pos$ 的编码线性变换得到。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$x$ 为当前位置；$k$ 为偏移量；$\cos(k), \sin(k)$ 为固定常数。
- **直觉理解**：模型可通过学习线性变换来"计算"相对位置关系，无需显式编码所有位置对。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-position-embedding-4)

来源：`topics/embedding/position-embedding/advanced-guide.md`

### 频率分析（公式 5）

<a id="formula-position-embedding-5-detail"></a>
$$\lambda_i = 2\pi \cdot 10000^{2i/d_{model}}$$

**详细讲解（小白友好版）**
- **一句话先懂**：不同维度的正弦波有不同的周期/波长。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$\lambda_i$ 为第 $i$ 维的波长；$2\pi$ 为正弦周期系数；$10000^{2i/d_{model}}$ 控制波长增长速度。
- **直觉理解**：低维度波长短（敏感于位置变化），高维度波长大（捕捉远距离关系）。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-position-embedding-5)

来源：`topics/embedding/position-embedding/advanced-guide.md`

### Learnable Position Embedding（公式 6）

<a id="formula-position-embedding-6-detail"></a>
$$E_{final} = E_{word} + E_{position}$$

**详细讲解（小白友好版）**
- **一句话先懂**：将词嵌入与位置嵌入相加得到最终输入表示。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$E_{word}$ 为词嵌入；$E_{position}$ 为可学习的位置嵌入；$L$ 为最大序列长度；$d$ 为维度。
- **直觉理解**：��置嵌入作为参数学习，能自适应数据分布，但长度受限于训练时的最大长度。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-position-embedding-6)

来源：`topics/embedding/position-embedding/advanced-guide.md`

### Shaw's Relative Position Encoding（公式 7）

<a id="formula-position-embedding-7-detail"></a>
$$e_{ij} = \frac{x_i W^Q (x_j W^K + a_{ij}^K)^T}{\sqrt{d_k}}$$

**详细讲解（小白友好版）**
- **一句话先懂**：在键向量上加上相对位置编码 $a_{ij}^K$，让注意力考虑位置关系。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$x_i, x_j$ 为位置 $i, j$ 的输入；$W^Q, W^K$ 为投影矩阵；$a_{ij}^K$ 为相对位置 $i-j$ 的编码。
- **直觉理解**：注意力分数不仅依赖内容相似度，还依赖相对位置关系。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-position-embedding-7)

来源：`topics/embedding/position-embedding/advanced-guide.md`

### T5 Bias（公式 8）

<a id="formula-position-embedding-8-detail"></a>
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}} + B\right) V$$

**详细讲解（小白友好版）**
- **一句话先懂**：在注意力分数上加一个相对位置偏置矩阵 $B$，再 softmax。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$QK^T/\sqrt{d_k}$ 为内容分数；$B$ 为相对位置偏置矩阵（$B_{ij}$ 依赖 $i-j$）。
- **直觉理解**：简单直接地让模型学习"距离越远/近"应该如何影响注意力。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-position-embedding-8)

来源：`topics/embedding/position-embedding/advanced-guide.md`

### RoPE (Rotary Position Embedding)（公式 9）

<a id="formula-position-embedding-9-detail"></a>
$$\text{RoPE}(x_m, m) = \begin{pmatrix} x_m^{(1)} \\ x_m^{(2)} \end{pmatrix} \otimes \begin{pmatrix} \cos(m\theta) \\ \sin(m\theta) \end{pmatrix}$$

**详细讲解（小白友好版）**
- **一句话先懂**：对向量每两维一组，乘以位置 $m$ 对应的旋转角度的正弦余弦。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$x_m^{(1)}, x_m^{(2)}$ 为第 $m$ 个位置的向量分量；$m\theta$ 为旋转角度；$\otimes$ 为逐元素乘。
- **直觉理解**：位置信息通过旋转角度注入，相对位置相同则内积也相同。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-position-embedding-9)

来源：`topics/embedding/position-embedding/advanced-guide.md`

### RoPE (Rotary Position Embedding)（公式 10）

<a id="formula-position-embedding-10-detail"></a>
$$\langle \text{RoPE}(x_m, m), \text{RoPE}(x_n, n) \rangle = \text{Re}[\langle x_m, x_n \rangle e^{i(m-n)\theta}]$$

**详细讲解（小白友好版）**
- **一句话先懂**：两个位置的 RoPE 编码的内积只依赖它们的相对位置 $m-n$。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$\langle \cdot, \cdot \rangle$ 为内积；$e^{i(m-n)\theta}$ 为复指数形式表示相对位置旋转。
- **直觉理解**：无论绝对位置在哪，只要相对距离相同，注意力分数的结构就相同。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-position-embedding-10)

来源：`topics/embedding/position-embedding/advanced-guide.md`

### ALiBi (Attention with Linear Biases)（公式 11）

<a id="formula-position-embedding-11-detail"></a>
$$\text{score}_{ij} = q_i \cdot k_j - m \cdot |i - j|$$

**详细讲解（小白友好版）**
- **一句话先懂**：在内容分数上减去与距离成正比的惩罚项。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$q_i \cdot k_j$ 为内容分数；$m$ 为每个头的斜率（控制距离衰减速度）；$|i-j|$ 为绝对距离。
- **直觉理解**：距离越远惩罚越大，模型自然偏好关注近处；无参数且外推能力强。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-position-embedding-11)

来源：`topics/embedding/position-embedding/advanced-guide.md`

## 全量微调

### 概述（公式 1）

<a id="formula-full-finetune-1-detail"></a>
$$W_{ft} = W_{pre} - \eta \sum_{t=1}^{T} \nabla_{W} \mathcal{L}(f_{W_{pre}}(x_t), y_t)$$

**详细讲解（小白友好版）**
- **一句话先懂**：从预训练权重出发，用目标任务数据做梯度下降更新所有参数。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$W_{pre}$ 为预训练权重；$\eta$ 为学习率；$\mathcal{L}$ 为损失函数；$T$ 为训练步数。
- **直觉理解**：全参数更新让模型充分适应新任务，效果通常最好但显存需求大。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-full-finetune-1)

来源：`topics/fine-tuning/full-finetune/advanced-guide.md`

### 问题定义（公式 2）

<a id="formula-full-finetune-2-detail"></a>
$$\text{Forgetting} = \text{Perf}_{pre}(T_{orig}) - \text{Perf}_{ft}(T_{orig})$$

**详细讲解（小白友好版）**
- **一句话先懂**：遗忘量 = 预训练模型在原任务上的性能 - 微调后在原任务上的性能。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$\text{Perf}_{pre}$ 为预训练模型性能；$\text{Perf}_{ft}$ 为微调后模型性能；$T_{orig}$ 为原任务。
- **直觉理解**：量化微调对原有能力的损害程度，值越大表示遗忘越严重。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-full-finetune-2)

来源：`topics/fine-tuning/full-finetune/advanced-guide.md`

### 显存估算（公式 3）

<a id="formula-full-finetune-3-detail"></a>
$$\text{Memory} \approx 4 \times \text{Params} \times \text{Bytes}$$

**详细讲解（小白友好版）**
- **一句话先懂**：全参数微调显存约为模型参数的 4 倍（以同精度计）。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$\text{Params}$ 为参数数量；$\text{Bytes}$ 为每个数值的字节数（如 FP16 为 2）。
- **直觉理解**：需要存储模型副本、梯度、优化器状态（如 Adam 的动量）和激活，总开销大。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-full-finetune-3)

来源：`topics/fine-tuning/full-finetune/advanced-guide.md`

## LoRA微调

### 概述（公式 1）

<a id="formula-lora-finetune-1-detail"></a>
$$W = W_0 + \Delta W = W_0 + BA$$

**详细讲解（小白友好版）**
- **一句话先懂**：冻结原权重 $W_0$，新增两个低秩矩阵 $B$ 和 $A$ 的乘积来表示权重更新。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$W_0$ 为预训练权重（冻结）；$B \in \mathbb{R}^{d \times r}$ 为降维矩阵；$A \in \mathbb{R}^{r \times k}$ 为升维矩阵；$r$ 为秩。
- **直觉理解**：用极少的可训练参数（$r \times (d+k)$）近似全参数更新（$d \times k$），大幅降低显存需求。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-lora-finetune-1)

来源：`topics/fine-tuning/lora-finetune/advanced-guide.md`

### 权重更新的低秩表示（公式 2）

<a id="formula-lora-finetune-2-detail"></a>
$$\Delta W = BA$$

**详细讲解（小白友好版）**
- **一句话先懂**：请结合该章节上下文理解该公式的计算目标。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：变量定义沿用原章节中的符号约定。
- **直觉理解**：该公式用于刻画该主题的核心机制与工程作用。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-lora-finetune-2)

来源：`topics/fine-tuning/lora-finetune/advanced-guide.md`

### 前向传播（公式 3）

<a id="formula-lora-finetune-3-detail"></a>
$$h = W_0 x + \frac{\alpha}{r} BAx$$

**详细讲解（小白友好版）**
- **一句话先懂**：输出 = 冻结权重的输出 + LoRA 更新的输出（带缩放因子）。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$W_0 x$ 为原模型输出；$BAx$ 为 LoRA 增量；$\alpha$ 为缩放因子；$r$ 为秩。
- **直觉理解**：$\alpha/r$ 控制 LoRA 更新的影响强度；除以 $r$ 让调 $\alpha$ 时无需因秩不同而重新调参。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-lora-finetune-3)

来源：`topics/fine-tuning/lora-finetune/advanced-guide.md`

### Alpha (α)（公式 4）

<a id="formula-lora-finetune-4-detail"></a>
$$\text{effective\_lr} \propto \frac{\alpha}{r} \times \text{learning\_rate}$$

**详细讲解（小白友好版）**
- **一句话先懂**：请结合该章节上下文理解该公式的计算目标。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：变量定义沿用原章节中的符号约定。
- **直觉理解**：该公式用于刻画该主题的核心机制与工程作用。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-lora-finetune-4)

来源：`topics/fine-tuning/lora-finetune/advanced-guide.md`

### 合并策略（公式 5）

<a id="formula-lora-finetune-5-detail"></a>
$$W_{merged} = W_0 + \frac{\alpha}{r} BA$$

**详细讲解（小白友好版）**
- **一句话先懂**：将 LoRA 的低秩更新合并到原权重，得到新的完整权重矩阵。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$W_0$ 为预训练权重；$B, A$ 为训练后的 LoRA 矩阵；$\alpha/r$ 为缩放因子。
- **直觉理解**：合并后模型与原模型结构相同，部署时无需 LoRA 代码，无额外推理开销。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-lora-finetune-5)

来源：`topics/fine-tuning/lora-finetune/advanced-guide.md`

## 模型量化

### 概述（公式 1）

<a id="formula-quantization-1-detail"></a>
$$\hat{W} = Q(W) = s \cdot \text{clamp}\left(\text{round}\left(\frac{W}{s}\right), -2^{b-1}, 2^{b-1}-1\right)$$

**详细讲解（小白友好版）**
- **一句话先懂**：将浮点数除以缩放因子、四舍五入、截断到目标范围，再乘回复原。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$s$ 为缩放因子（scale）；$b$ 为目标位宽；$\text{round}$ 为舍入；$\text{clamp}$ 为截断到 $[-2^{b-1}, 2^{b-1}-1]$。
- **直觉理解**：用有限的整数近似连续浮点数，牺牲少量精度换取大幅压缩。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-quantization-1)

来源：`topics/inference/quantization/advanced-guide.md`

### 1. 对称量化（公式 2）

<a id="formula-quantization-2-detail"></a>
$$s = \frac{\max(|W|)}{2^{b-1}}$$

**详细讲解（小白友好版）**
- **一句话先懂**：请结合该章节上下文理解该公式的计算目标。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：变量定义沿用原章节中的符号约定。
- **直觉理解**：该公式用于刻画该主题的核心机制与工程作用。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-quantization-2)

来源：`topics/inference/quantization/advanced-guide.md`

### 1. 对称量化（公式 3）

<a id="formula-quantization-3-detail"></a>
$$Q(x) = s \cdot \text{round}\left(\frac{x}{s}\right)$$

**详细讲解（小白友好版）**
- **一句话先懂**：缩放因子由最大绝对值决定，量��范围关于 0 对称。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$s$ 为缩放因子；$b$ 为位宽；$2^{b-1}$ 为量化级别数的一半。
- **直觉理解**：零点保持不变，实现简单，但非均匀分布时会浪费部分量化范围。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-quantization-3)

来源：`topics/inference/quantization/advanced-guide.md`

### 2. 非对称量化（公式 4）

<a id="formula-quantization-4-detail"></a>
$$s = \frac{\max(W) - \min(W)}{2^b - 1}$$

**详细讲解（小白友好版）**
- **一句话先懂**：请结合该章节上下文理解该公式的计算目标。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：变量定义沿用原章节中的符号约定。
- **直觉理解**：该公式用于刻画该主题的核心机制与工程作用。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-quantization-4)

来源：`topics/inference/quantization/advanced-guide.md`

### 2. 非对称量化（公式 5）

<a id="formula-quantization-5-detail"></a>
$$z = \text{round}\left(-\frac{\min(W)}{s}\right)$$

**详细讲解（小白友好版）**
- **一句话先懂**：请结合该章节上下文理解该公式的计算目标。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：变量定义沿用原章节中的符号约定。
- **直觉理解**：该公式用于刻画该主题的核心机制与工程作用。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-quantization-5)

来源：`topics/inference/quantization/advanced-guide.md`

### 2. 非对称量化（公式 6）

<a id="formula-quantization-6-detail"></a>
$$Q(x) = s \cdot (\text{round}(x/s) + z)$$

**详细讲解（小白友好版）**
- **一句话先懂**：缩放因子由值域范围决定，引入零点偏移 $z$ 适配非对称分布。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$s$ 为缩放因子；$z$ 为零点（zero point）；$2^b - 1$ 为量化级别数。
- **直觉理解**：充分利用整个量化范围，精度更高，但需要额外存储零点。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-quantization-6)

来源：`topics/inference/quantization/advanced-guide.md`

### 原理（公式 7）

<a id="formula-quantization-7-detail"></a>
$$\arg\min_{\hat{W}} \|WX - \hat{W}X\|^2$$

**详细讲解（小白友好版）**
- **一句话先懂**：寻找量化后的���重 $\hat{W}$，使其输出与原始权重 $W$ 的输出尽可能接近。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$W$ 为原始权重；$\hat{W}$ 为量化后权重；$X$ 为输入激活；$\|\cdot\|^2$ 为 L2 范数平方。
- **直觉理解**：直接优化任务相关误差（输出差异），而非单纯的权重误差。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-quantization-7)

来源：`topics/inference/quantization/advanced-guide.md`

### 算法流程（公式 8）

<a id="formula-quantization-8-detail"></a>
   $$W_{[:, i+1:]} \leftarrow W_{[:, i+1:]} - \frac{\delta_i \cdot (X_i^T X_{[i+1:]})}{X_i^T X_i + \epsilon}$$

**详细讲解（小白友好版）**
- **一句话先懂**：将当前列的量化误差通过 Hessian 逆矩阵传播到后续未量化的列。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$\delta_i$ 为第 $i$ 列的量化误差；$X_i^T X_{[i+1:]}$ 为激活协方差；$\epsilon$ 为数值稳定性常数。
- **直觉理解**：用后续列的调整来补偿当前列量化带来的输出误差。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-quantization-8)

来源：`topics/inference/quantization/advanced-guide.md`

### 算法（公式 9）

<a id="formula-quantization-9-detail"></a>
$$\hat{W} = Q(W \cdot s) \cdot s^{-1}$$

**详细讲解（小白友好版）**
- **一句话先懂**：先对权重做缩放，量化后再逆向缩放，使重要权重获得更高精度。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$W$ 为原始权重；$s$ 为每通道缩放因子；$Q(\cdot)$ 为量化函数；$s^{-1}$ 为逐元素除法。
- **直觉理解**：缩放后重要权重的值域扩大，��化时被分配更多量化级别。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-quantization-9)

来源：`topics/inference/quantization/advanced-guide.md`

### 算法（公式 10）

<a id="formula-quantization-10-detail"></a>
$$s_i = \max(|X_i|)^\alpha, \quad \alpha \in [0, 1]$$

**详细讲解（小白友好版）**
- **一句话先懂**：缩放因子由激活值的幅值决定，激活越大的通道权重越重要。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$X_i$ 为第 $i$ 通道的激活值；$\alpha$ 为调节幂次（通常接近 1）。
- **直觉理解**：激活大的通道对输出影响更大，应保留更高精度的权重。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-quantization-10)

来源：`topics/inference/quantization/advanced-guide.md`

## 推理加速

### 原理（公式 1）

<a id="formula-inference-acceleration-1-detail"></a>
$$\text{Attention}(Q_t, K_{1:t}, V_{1:t})$$

**详细讲解（小白友好版）**
- **一句话先懂**：生成第 $t$ 个 token 时，需要当前查询 $Q_t$ 与所有历史键值 $K_{1:t}, V_{1:t}$ 计算注意力。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$Q_t$ 为当前位置的查询向量；$K_{1:t}, V_{1:t}$ 为从位置 1 到 $t$ 的键值序列。
- **直觉理解**：每生成一个 token 都要"看"之前所有位置，计算量随长度线性增长。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-inference-acceleration-1)

来源：`topics/inference/inference-acceleration/advanced-guide.md`

### 内存需求（公式 2）

<a id="formula-inference-acceleration-2-detail"></a>
$$\text{KV Cache} = 2 \times L \times h \times d_{head} \times s \times \text{bytes}$$

**详细讲解（小白友好版）**
- **一句话先懂**：KV Cache 大小与层数、头数、每头维度、序列长度成正比。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$L$ 为层数；$h$ 为注意力头数；$d_{head}$ 为每头维度；$s$ 为序列长度；bytes 为每个数值的字节数。
- **直觉理解**：序列越长缓存越大，是长上下文推理的主要内存瓶颈；2 表示 K 和 V 两份缓存。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-inference-acceleration-2)

来源：`topics/inference/inference-acceleration/advanced-guide.md`

### 标准注意力（公式 3）

<a id="formula-inference-acceleration-3-detail"></a>
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right) V$$

**详细讲解（小白友好版）**
- **一句话先懂**：计算 Q 与 K 的相似度，缩放后 softmax 得到权重，再对 V 加权求和。
- **分步理解**：你可以把这条公式看成 3 步：先拿到输入；再按公式里的规则做运算（加减乘除、归一化、加权等）；最后得到输出结果。
- **变量白话**：$Q, K, V$ 为查询、键、值矩阵；$d$ 为键维度；$\sqrt{d}$ 为缩放因子。
- **直觉理解**：核心注意力公式，但需要存储 $N \times N$ 的注意力矩阵，内存开销大。
- **生活类比**：把它想成“做菜配方”。输入是食材，参数是调料比例，公式是做菜步骤，输出就是成品味道。
- **实际有什么用**：它帮助模型决定“哪些信息该放大、哪些该忽略”，所以会直接影响训练稳定性、收敛速度和最终效果。
- **给新手的记忆法**：先记这条公式解决什么问题，再记最关键的 2-3 个符号，最后再看完整写法，不要一次硬背全部符号。
[↩ 返回原位置](#formula-inference-acceleration-3)

来源：`topics/inference/inference-acceleration/advanced-guide.md`

